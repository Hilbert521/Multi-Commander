# 流式机器学习

## 流式计算

弄明白流计算，首先要搞清楚概念。先来看下流计算（stream compute）以及批计算（batch compute）的计算模型：

1. 流计算：当一条数据被处理完成后，序列化到缓存中,然后立刻通过网络传输到下一个节点，由下一个节点继续处理。
2. 批处理系统：当一条数据被处理完成后，序列化到缓存中,并不会立刻通过网络传输到下一个节点，当缓存写满,就持久化到本地硬盘上，当所有数据都被处理完成后，才开始将处理后的数据通过网络传输到下一个节点。

## FTRL算法

### 概述

常见的两种损失函数的计算方法如下：
$$
\begin{array}{l}{\ell(W, Z)=\sum_{j=1}^{M}\left(y_{j}-W^{T} X_{j}\right)^{2}} \\ {\ell(W, Z)=\sum_{j=1}^{M} \log \left(1+\exp \left(-y_{j} W^{T} X_{j}\right)\right)}\end{array}
$$
流式算法跟批算法最大的不同就在于数据的计算量，批算法和流算法的计算模型如下：

- 批算法：每次使用全量数据计算损失函数和梯度，然后更新模型
- 流算法：每次使用1条数据计算损失函数和梯度，然后更新模型

从这个角度来看，因为每次参与训练的数据量变小了，所以对于算法从训练**数据稀疏度**和**数据维度**这两给角度来看，有更多的约束和挑战。

### FTRL推导





1. 1000 episode 之后加载test flow
2. 改新的强化学习方法
3. 进行在线提升
4. 新强化学习要求：
   - 保证不会比baseline差
   - 实时强化
   - on-policy



1. 在模拟器中训练之后，应用到现实，在现实中随机应变
2. 模仿rule based method，简化训练过程
3. test数据 1 个episode



test

1. 1 episode 1 timestep
2. 1 timestep 输出 1 reward
3. reward tensorboard
4. 3000 timestep
5. sarsa state 下可以输出自己的action
6. QLearning网络值加载到sarsa